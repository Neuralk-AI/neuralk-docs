"use strict";(globalThis.webpackChunkdocs_neuralk=globalThis.webpackChunkdocs_neuralk||[]).push([[1071],{5235:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"classification","title":"Model tutorial: Classification","description":"With the Neuralk API you can run our foundation models like NICL on classification tasks across any tabular dataset containing textual and numerical features.","source":"@site/docs/classification.mdx","sourceDirName":".","slug":"/classification","permalink":"/docs/classification","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Usage guide","permalink":"/docs/models"},"next":{"title":"Overview","permalink":"/docs/expert-modules"}}');var i=t(4848),s=t(8453);t(4783),t(9489),t(7227);const r={sidebar_position:2},o="Model tutorial: Classification",c={},l=[{value:"Import required libraries",id:"import-required-libraries",level:2},{value:"Load credentials (see Quickstart for more)",id:"load-credentials-see-quickstart-for-more",level:2},{value:"Load your dataset",id:"load-your-dataset",level:2},{value:"Classify your dataset",id:"classify-your-dataset",level:2},{value:"Type 1: General classification",id:"type-1-general-classification",level:3},{value:"Type 2: Classification on textual data",id:"type-2-classification-on-textual-data",level:3},{value:"Type 3: Classification on categorical data",id:"type-3-classification-on-categorical-data",level:3},{value:"Advanced: Classification on large datasets with context sampling",id:"advanced-classification-on-large-datasets-with-context-sampling",level:3}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"model-tutorial-classification",children:"Model tutorial: Classification"})}),"\n",(0,i.jsxs)(n.p,{children:["With the Neuralk API you can run our foundation models like ",(0,i.jsx)(n.a,{href:"/docs/models#nicl-neuralk-in-context-learning-icl",children:"NICL"})," on classification tasks across any tabular dataset containing textual and numerical features."]}),"\n",(0,i.jsx)(n.h2,{id:"import-required-libraries",children:"Import required libraries"}),"\n",(0,i.jsx)(n.p,{children:"The below libraries are required to run the classification tutorial examples."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import os\n\nimport numpy as np\n\nimport polars as pl\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\n\n# Neuralk imports\nfrom neuralk import Classifier\n"})}),"\n",(0,i.jsxs)(n.h2,{id:"load-credentials-see-quickstart-for-more",children:["Load credentials (see ",(0,i.jsx)(n.a,{href:"/docs/quickstart",children:"Quickstart"})," for more)"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'try:\n    from dotenv import load_dotenv\n    load_dotenv()\nexcept ImportError:\n    print("python-dotenv not installed, skipping .env loading")\n\nuser = os.environ.get("NEURALK_USERNAME")\npassword = os.environ.get("NEURALK_PASSWORD")\n\nassert (\n    user is not None and password is not None\n), "Missing NEURALK_USER or NEURALK_PASSWORD. Set them in your environment or a .env file."\n'})}),"\n",(0,i.jsx)(n.h2,{id:"load-your-dataset",children:"Load your dataset"}),"\n",(0,i.jsxs)(n.p,{children:["For this example we use a synthetically generated dataset from the ",(0,i.jsx)(n.code,{children:"skrub"})," library."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-Python",children:"\nX, y = make_classification(\n    n_samples=1000, \n    n_features=10,\n    n_informative=8,\n    n_classes=3,\n    random_state=42\n)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"classify-your-dataset",children:"Classify your dataset"}),"\n",(0,i.jsx)(n.h3,{id:"type-1-general-classification",children:"Type 1: General classification"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f"Training set size: {X_train.shape[0]}")\nprint(f"Test set size: {X_test.shape[0]}")\nprint(f"Number of features: {X_train.shape[1]}")\nprint(f"Number of classes: {len(np.unique(y))}")\n\n# Train the classifier\nclassifier = Classifier()\nclassifier.fit(X_train, y_train)\nprint("\u2713 Classifier fitted successfully - In our case, this only means saving the X_train and y_train in the classifier object for the inference")\n\n# Make predictions\npredictions = classifier.predict(X_test)\n\n# Evaluate performance\nacc = accuracy_score(y_test, predictions)\nprint(f"\u2713 Accuracy: {acc:.3f}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"type-2-classification-on-textual-data",children:"Type 2: Classification on textual data"}),"\n",(0,i.jsx)(n.p,{children:"Since NICL operates on numeric vectors, any textual data in your dataset must first be converted into numerical features compatible with NICL."}),"\n",(0,i.jsx)(n.p,{children:"Each text sample should first be converted into a dense representation using a pre-trained sentence embedding model."}),"\n",(0,i.jsx)(n.p,{children:"These embeddings are then compressed using PCA to a smaller dimension to improve computation efficiency."}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"We recommend using PCA while retaining the first 40 components. This encoding ensures that the inputs meet NICL\u2019s size requirements in most cases."})}),"\n",(0,i.jsx)(n.p,{children:"For this example we use a synthetically generated dataset."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\ntext_templates = [\n        "This is a product description for electronics category",\n        "Customer review about home and garden items",\n        "Technical specification for automotive parts",\n        "Food and beverage product information"\n]\n\n# Generate random text data (only for this tutorial)\nn_samples = X.shape[0]\ntexts = np.random.choice(text_templates, size=n_samples).reshape(-1, 1)\n\n# Combine numeric features with text in a Polars DataFrame\n\nX_text = pl.DataFrame(np.concatenate([X, texts], axis=1), schema=[f"f_{i}" for i in range(X.shape[1])] + ["text"])\n\n# Split\nX_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n        X_text, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f"Training set size: {len(X_train_text)}")\nprint(f"Test set size: {len(X_test_text)}")\nprint(f"Number of classes: {len(np.unique(y_train_text))}")\n\nenc = TextEncoder(model_name=\'intfloat/e5-small-v2\', n_components=40)\n\n# Column transformer: encode text only, scale numeric if needed\npreprocessor = ApplyToCols(\n        enc,\n        cols=["text"],\n        allow_reject=True\n)\n\n# Fit-transform train, transform test\nX_train_encoded = preprocessor.fit_transform(X_train_text, y_train_text)\nX_test_encoded = preprocessor.transform(X_test_text)\n\nprint(f"\u2713 Text encoded to {X_train_encoded.shape[1]} features")\n\n# Train classifier on sampled data\nprint("\\n\ud83e\udd16 Training classifier...")\nnicl_classifier = Classifier()\nnicl_classifier.fit(X_train_encoded, y_train_text)\nprint("\u2713 Text classifier fitted successfully")\n\n# Make predictions\nprint("\\n\ud83d\udd2e Making predictions...")\npredictions = nicl_classifier.predict(X_test_encoded)\n\n# Evaluate performance\ntext_acc = accuracy_score(y_test_text, predictions)\nprint(f"\u2713 Accuracy: {text_acc:.3f}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"type-3-classification-on-categorical-data",children:"Type 3: Classification on categorical data"}),"\n",(0,i.jsxs)(n.p,{children:["For categorical features, we use an ",(0,i.jsx)(n.code,{children:"OrdinalEncoder"})," instead of a one-hot encoder to maintain control over input dimensionality. While one-hot encoding creates a new binary column for each possible category the ordinal approach assigns a unique integer to each category within a feature."]}),"\n",(0,i.jsx)(n.p,{children:"This representation ensures that NICL receives a fixed number of inputs."}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"The preprocessing pipeline presented here represents a minimal set of steps to ensure compatibility with the model\u2019s input expectations, such as appropriate numerical ranges, feature types, and dimensional balance. It is designed to provide stable and interpretable inputs that allow the model to function correctly."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\n# Create synthetic mixed dataset (numerical + categorical) - only for this tutorial \nnp.random.seed(42)\n\nn_samples = X.shape[0]\n\n# Generate random categorical features (only for this tutorial)\ncategories_a = np.random.choice([\'A\', \'B\', \'C\'], n_samples).reshape(-1, 1)\ncategories_b = np.random.choice([\'X\', \'Y\', \'Z\', \'W\'], n_samples).reshape(-1, 1)\n\nX_with_categories = pl.DataFrame(np.concatenate([X, categories_a, categories_b], axis=1))\n\nX_train_with_categories, X_test_with_categories, y_train_with_categories, y_test_with_categories = train_test_split(\nX_with_categories, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Setup preprocessing pipeline\nprint("\\n\ud83d\udd27 Setting up preprocessing pipeline...")\n\nvec = TableVectorizer(\n    numeric=Pipeline([\n        ("imputer", SimpleImputer(strategy="mean")),\n        ("scaler", StandardScaler())\n    ]),\n    low_cardinality=Pipeline([\n        ("imputer", SimpleImputer(strategy="most_frequent")),\n        ("encoder", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1))\n    ])\n)\n\n# Fit and transform the data\nX_train_processed = vec.fit_transform(X_train_with_categories)\nX_test_processed = vec.transform(X_test_with_categories)\n\nprint(f"\u2713 Transformed shape: {X_train_processed.shape}")\n\n# Train classifier\nprint("\\n\ud83e\udd16 Training classifier...")\nnicl_classifier = Classifier()\nnicl_classifier.fit(X_train_processed, y_train_with_categories)\nprint("\u2713 Mixed data classifier fitted successfully")\n\n# Make predictions\nprint("\\n\ud83d\udd2e Making predictions...")\npredictions = nicl_classifier.predict(X_test_processed)\n# Evaluate performance\ncateg_acc = accuracy_score(y_test_with_categories, predictions)\nprint(f"\u2713 Accuracy: {categ_acc:.3f}")\n\n'})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-classification-on-large-datasets-with-context-sampling",children:"Advanced: Classification on large datasets with context sampling"}),"\n",(0,i.jsx)(n.p,{children:"When working with very large datasets, training can become computationally expensive and time-consuming."}),"\n",(0,i.jsx)(n.p,{children:"To manage this, it is often advised to apply row sampling, selecting a representative subset of the data to train the model correctly while preserving its generalisation capability."}),"\n",(0,i.jsx)(n.p,{children:"In our example, we illustrate this using random sampling of what we refer to as the context (the portion of the dataset used for model train input)."}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"This approach helps balance performance and computational feasibility. As with other preprocessing steps, it is recommended to experiment with different sampling strategies and proportions to determine what best fits the data characteristics and available computational resources."})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'\n# Create large synthetic dataset (only for this tutorial)\nX, y = make_classification(\n    n_samples=10000, \n    n_features=10,\n    n_informative=8,\n    n_classes=3,\n    random_state=42\n)\n\n# Sampling Context Methods Example\nprint("=" * 60)\nprint("SAMPLING CONTEXT METHODS")\nprint("=" * 60)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclasses = np.unique(y_train)\nn_classes = len(classes)\nn_samples = max(int(X_train.shape[0] * 0.2), n_classes * 2)\n\n# We ensure at least one element per class\nselected_indices = []\nfor c in classes:\n    class_indices = np.where(y_train == c)[0]\n    chosen = np.random.choice(class_indices, 1, replace=False)\n    selected_indices.extend(chosen)\n\n# We sample the remaining indices\nremaining_indices = np.setdiff1d(np.arange(len(y_train)), selected_indices)\nn_remaining = n_samples - n_classes\nextra_indices = np.random.choice(remaining_indices, n_remaining, replace=False)\nselected_indices.extend(extra_indices)\n\nsampled_X_train, sampled_y_train = X_train[selected_indices], y_train[selected_indices]\n\nprint(f"Original training set size: {X_train.shape[0]}")\nprint(f"Sampled training set size: {sampled_X_train.shape[0]}")\nprint(f"Sampling ratio: {sampled_X_train.shape[0] / X_train.shape[0]:.2%}")\n\n# Run classification\nnicl_classifier = Classifier()\nnicl_classifier.fit(sampled_X_train, sampled_y_train)\npredictions = nicl_classifier.predict(X_test)\nacc = accuracy_score(y_test, predictions)\nprint(f\'Accuracy: {acc:.3f}\')\n'})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"As with any machine learning workflow, these steps should be viewed as a starting point rather than a fixed recipe. It is recommended to experiment with different preprocessing strategies, such as alternative encoders, scaling methods, or feature selection techniques, to identify the configuration that yields the best performance for the specific data and task at hand."})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},7227:(e,n,t)=>{t.d(n,{A:()=>r});t(6540);var a=t(4164);const i={tabItem:"tabItem_Ymn6"};var s=t(4848);function r({children:e,hidden:n,className:t}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(i.tabItem,t),hidden:n,children:e})}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}},9489:(e,n,t)=>{t.d(n,{A:()=>j});var a=t(6540),i=t(4164),s=t(8630),r=t(4245),o=t(6347),c=t(6494),l=t(2814),d=t(5167),p=t(9900);function u(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function h({value:e,tabValues:n}){return n.some(n=>n.value===e)}function f({queryString:e=!1,groupId:n}){const t=(0,o.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,l.aZ)(i),(0,a.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(t.location.search);n.set(i,e),t.replace({...t.location,search:n.toString()})},[i,t])]}function _(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,s=m(e),[r,o]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!h({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:s})),[l,d]=f({queryString:t,groupId:i}),[u,_]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,i]=(0,p.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),g=(()=>{const e=l??u;return h({value:e,tabValues:s})?e:null})();(0,c.A)(()=>{g&&o(g)},[g]);return{selectedValue:r,selectValue:(0,a.useCallback)(e=>{if(!h({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);o(e),d(e),_(e)},[d,_,s]),tabValues:s}}var g=t(1062);const y={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(4848);function b({className:e,block:n,selectedValue:t,selectValue:a,tabValues:s}){const o=[],{blockElementScrollPositionUntilNextRender:c}=(0,r.a_)(),l=e=>{const n=e.currentTarget,i=o.indexOf(n),r=s[i].value;r!==t&&(c(n),a(r))},d=e=>{let n=null;switch(e.key){case"Enter":l(e);break;case"ArrowRight":{const t=o.indexOf(e.currentTarget)+1;n=o[t]??o[0];break}case"ArrowLeft":{const t=o.indexOf(e.currentTarget)-1;n=o[t]??o[o.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:a})=>(0,x.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{o.push(e)},onKeyDown:d,onClick:l,...a,className:(0,i.A)("tabs__item",y.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function v({lazy:e,children:n,selectedValue:t}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function w(e){const n=_(e);return(0,x.jsxs)("div",{className:(0,i.A)(s.G.tabs.container,"tabs-container",y.tabList),children:[(0,x.jsx)(b,{...n,...e}),(0,x.jsx)(v,{...n,...e})]})}function j(e){const n=(0,g.A)();return(0,x.jsx)(w,{...e,children:u(e.children)},String(n))}}}]);